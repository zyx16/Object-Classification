{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from option.option import parse\n",
    "from data import create_dataset, create_dataloader\n",
    "from model import define_C\n",
    "from solver import create_solver\n",
    "from util.metric import MetricMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Path [/home/tsinghuaee01/projects/Object-Classification/experiments/densenet161_object_classification] already exists. Rename it to [/home/tsinghuaee01/projects/Object-Classification/experiments/densenet161_object_classification_archived_190511-112235]\n",
      "[Warning] Path [/home/tsinghuaee01/projects/Object-Classification/tb_logger/densenet161_object_classification] already exists. Rename it to [/home/tsinghuaee01/projects/Object-Classification/tb_logger/densenet161_object_classification_archived_190511-112235]\n",
      "export CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "opt = parse('./option/train/train_dense161.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phase, dataset_opt in opt['datasets'].items():\n",
    "    if phase == 'train':\n",
    "        train_set = create_dataset(dataset_opt)\n",
    "        train_loader = create_dataloader(train_set, dataset_opt)\n",
    "    elif phase == 'val':\n",
    "        val_set = create_dataset(dataset_opt)\n",
    "        val_loader = create_dataloader(val_set, dataset_opt)\n",
    "    else:\n",
    "        raise NotImplementedError('Dataset phase %s is not implemented!' % (phase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Using sample balance, weights are\n",
      "tensor([ 2.0275, 15.4964,  8.8093, 30.4253,  7.1323, 25.0112, 27.0040, 16.0639,\n",
      "        19.1304, 23.6277, 21.7705,  9.1535, 21.7705, 17.9754, 14.3991,  7.5424,\n",
      "        15.3797, 19.4265, 14.4677, 18.5634])\n"
     ]
    }
   ],
   "source": [
    "if opt['solver']['balance_sample']:\n",
    "    pos_weight = torch.zeros(20).float()\n",
    "    for info in train_set.info_list:\n",
    "        pos_weight[info[1]] += 1\n",
    "    pos_weight = (len(train_set) - pos_weight) / pos_weight\n",
    "    opt['solver']['pos_weight'] = pos_weight\n",
    "    print('===> Using sample balance, weights are')\n",
    "    print(pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7130, 25.4694, 14.5100, 50.0660, 12.1864, 41.6220, 44.2971, 27.4895,\n",
       "        31.5105, 39.5468, 36.0753, 15.3535, 35.5743, 29.1560, 22.9513, 12.6692,\n",
       "        25.5343, 32.0061, 24.0602, 30.8412])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> tensorboardX logger created, log to /home/tsinghuaee01/projects/Object-Classification/tb_logger/densenet161_object_classification\n"
     ]
    }
   ],
   "source": [
    "if opt['use_tb_logger']:\n",
    "    from tensorboardX import SummaryWriter\n",
    "    tb_logger = SummaryWriter(log_dir=opt['path']['tb_logger'])\n",
    "    print('===> tensorboardX logger created, log to %s' %\n",
    "          (opt['path']['tb_logger']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "===> Network Summary\n",
      "\n",
      "Network structure: [DataParallel - densenet161], with parameters: [26,516,180]\n",
      "==================================================\n",
      "===> Solver Initialized : [OCSolver] || Use GPU : [True]\n",
      "optimizer_C:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0001\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "lr_scheduler milestones: [10, 15, 18]   gamma: 0.500000\n"
     ]
    }
   ],
   "source": [
    "solver = create_solver(opt)\n",
    "model_name = opt['network_C']['which_model_C'].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Start Train\n",
      "==================================================\n",
      "Method: DENSENET161 || Epoch Range: (1 ~ 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('===> Start Train')\n",
    "print(\"==================================================\")\n",
    "\n",
    "solver_log = solver.get_current_log()\n",
    "\n",
    "NUM_EPOCH = int(opt['solver']['epoch'])\n",
    "start_epoch = solver_log['epoch']\n",
    "current_step = solver_log['step']\n",
    "\n",
    "print(\"Method: %s || Epoch Range: (%d ~ %d) || Start Step: %d\" %\n",
    "      (model_name, start_epoch, NUM_EPOCH, current_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    metric_meter = MetricMeter(class_num=20)\n",
    "    for epoch in range(start_epoch, NUM_EPOCH + 1):\n",
    "        metric_meter.reset()\n",
    "        print('\\n===> Training Epoch: [%d/%d]...  Learning Rate: %f' %\n",
    "              (epoch, NUM_EPOCH, solver.get_current_learning_rate()))\n",
    "\n",
    "        # Initialization\n",
    "        solver_log['epoch'] = epoch\n",
    "\n",
    "        # Train model\n",
    "        train_loss_dict = {}\n",
    "        val_loss_dict = {}\n",
    "        for k in solver_log['records'].keys():\n",
    "            if k.startswith('train'):  # 'train_loss_pixel'\n",
    "                train_loss_dict[k[6:]] = []\n",
    "            elif k.startswith('val'):\n",
    "                val_loss_dict[k[4:]] = []\n",
    "                \n",
    "        with tqdm(\n",
    "                total=len(train_loader),\n",
    "                desc='Epoch: [%d/%d]' % (epoch, NUM_EPOCH),\n",
    "                miniters=1) as t:\n",
    "            for iter, batch in enumerate(train_loader):\n",
    "                current_step += 1\n",
    "                solver.feed_data(batch)\n",
    "                iter_loss = solver.train_step()\n",
    "                batch_size = batch['img'].size(0)\n",
    "                for k, v in iter_loss.items():\n",
    "                    train_loss_dict[k].append(v * batch_size)\n",
    "                if opt['use_tb_logger']:\n",
    "                    if current_step % opt['logger']['print_freq'] == 0:\n",
    "                        for k, v in iter_loss.items():\n",
    "                            tb_logger.add_scalar('train_' + k, v, current_step)\n",
    "\n",
    "                t.set_postfix_str(\"Batch Loss: %.4f\" % iter_loss['loss_total'])\n",
    "                t.update()\n",
    "\n",
    "        for k, v in train_loss_dict.items():\n",
    "            solver_log['records']['train_' + k].append(sum(v) / len(v))\n",
    "        solver_log['records']['lr'].append(solver.get_current_learning_rate())\n",
    "\n",
    "        print(\n",
    "            '\\nEpoch: [%d/%d]   Avg Train Loss: %.6f' %\n",
    "            (epoch, NUM_EPOCH, solver_log['records']['train_loss_total'][-1]))\n",
    "\n",
    "        print('===> Validating...', )\n",
    "        \n",
    "        for iter, batch in enumerate(val_loader):\n",
    "            solver.feed_data(batch)\n",
    "            iter_loss = solver.test()\n",
    "            for k, v in iter_loss.items():\n",
    "                val_loss_dict[k].append(v)\n",
    "            metric_meter.add(solver.predict, solver.target)\n",
    "            \n",
    "        for k, v in val_loss_dict.items():\n",
    "            solver_log['records']['val_' + k].append(sum(v) / len(v))\n",
    "        metric_value = metric_meter.value()\n",
    "        for k, v in metric_value.items():\n",
    "            solver_log['records'][k].append(v)\n",
    "            \n",
    "        if opt['use_tb_logger']:\n",
    "            for k, v in iter_loss.items():\n",
    "                tb_logger.add_scalar('val_' + k, v, current_step)\n",
    "            for k, v in metric_value.items():\n",
    "                tb_logger.add_scalar('val_' + k, v, current_step)\n",
    "                \n",
    "        # record the best epoch\n",
    "        epoch_is_best = False\n",
    "        if solver_log['best_mAP'] < metric_value['mAP']:\n",
    "            solver_log['best_mAP'] = metric_value['mAP']\n",
    "            epoch_is_best = True\n",
    "            solver_log['best_epoch'] = epoch\n",
    "\n",
    "        print(\n",
    "            \"[%s] mAP: %.2f   mAcc: %.4f   wAcc: %.4f Loss: %.6f   Best mAP: %.2f in Epoch: [%d]\"\n",
    "            % (val_set.name(), metric_value['mAP'],\n",
    "               metric_value['mAcc'], metric_value['wAcc'],\n",
    "               solver_log['records']['val_loss_total'][-1],\n",
    "               solver_log['best_mAP'], solver_log['best_epoch']))\n",
    "\n",
    "        solver.set_current_log(solver_log)\n",
    "        solver.save_checkpoint(epoch, epoch_is_best)\n",
    "        solver.save_current_log()\n",
    "\n",
    "        # update lr\n",
    "        solver.update_learning_rate(epoch)\n",
    "\n",
    "    print('===> Finished !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_c': tensor(0.3805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(it)\n",
    "solver.feed_data(sample)\n",
    "solver.train_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'afas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.startswith('a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
